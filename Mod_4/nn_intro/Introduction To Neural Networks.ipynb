{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center\">Introduction to Neural Networks and The Preceptron</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style='width:300px' src='images/move_on.jpg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Neural Network Lecture Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### 1. Introduction to Neural Networks and The Perceptron \n",
    "##### 2. Multilayer Perceptron and Deeper Neural Networks \n",
    "        - Application : Classifier with Keras\n",
    "##### 3. Convolutional Neural Networks\n",
    "        - Application: CNN image classifier with Keras and CoLab "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Applications of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Clustering\n",
    "- Pattern Recognition\n",
    "- Image Recognition (CNN)\n",
    "- Time Series Forecasting (RNN)\n",
    "- Audio/Video/Image Generation (GAN) \n",
    "\n",
    "LIMITATIONS\n",
    "- Good for prediction bad for inference \n",
    "- Computationally expensive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The perceptron algorithm is about learning the weights for the inputs in order to draw linear decision boundary that allows us to discriminate between the two linearly separable classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src='images/perceptron_binary.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### McCulloch-Pitts Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The idea for a perceptron (aka a neuron the building block of neural networks) was formulated in 1943 by Warren McCulloch and Walter Pitts. Modeled after the neurons the brain they proposed a mathematical process that can take in a set of inputs, apply some sort of aggregation function <i>(g)</i> on the inputs and subsequently apply a decision function/<b>activation function</b> <i>(f)</i> on the aggregation. If the aggregation is greater than a certain threshold the process returns a 1, else a 0. This can be used to for a binary classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src='images/mcCulloch_Pitts.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This model required manually determining the threshold and each input was given equal importance, so a few years later Frank Rosenblatt proposed a model that could \"learn\" the threshold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Frank Rosenblatt's Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src='images/rosenblatt_perceptron_schematic.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Rosenblatt introduced the concept of weights (w) being applied to each input with larger weights giving more importance to the feature it is applied to. These weights can then be updated based on whether the perceptron made a correct classification or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<h3>Unit Step Function</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Before diving into the process of updating weights let's cover how the basic decision function in a perceptron works: \n",
    "If the sum of the inputs multiplied by their corresponding weights is greater than a certain threshold the function returns a 1 otherwise it returns a 0 (or -1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Aggregation : \n",
    "<p style='font-size:24px'>$z = w_{1}x_{1}... + w_{n}x_{n}$</p>\n",
    "\n",
    "Decision : \n",
    "<p style='font-size:24px'>$ g(z) =\n",
    "  \\begin{cases}\n",
    "    1       & \\quad \\text{if } n > \\theta \\\\\n",
    "    0  & \\quad \\text   otherwise\n",
    "  \\end{cases}\n",
    "$</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### How do we determine the best <b style=\"font-size: 16px\">$\\theta$</b> aka threshold (also written as $w_{0}$)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we move <b style=\"font-size: 16px\">$\\theta$</b> to other side as a weight to an input of 1 we can update this too based on if the perceptron is able to classify things correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Aggregation : \n",
    "<p style='font-size:24px'>$z = \\theta * 1 + w_{1}x_{1}... + w_{n}x_{n}$</p>\n",
    "<p style='font-size:24px'>$z = w_{0} * 1 + w_{1}x_{1}... + w_{n}x_{n}$</p>\n",
    "\n",
    "Decision : \n",
    "<p style='font-size:24px'>$ g(z) =\n",
    "  \\begin{cases}\n",
    "    1       & \\quad \\text{if } n > 0 \\\\\n",
    "    0  & \\quad \\text   otherwise\n",
    "  \\end{cases}\n",
    "$</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<h4>Unit Step Function aka Heaveside Step Function</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src='images/unit_step.svg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<h3>Perceptron Learning</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A perceptron learns by updating the weights it applies to its inputs. The algorithm functions as follows : \n",
    "\n",
    "1. Initialize the weights to 0 or small random numbers.\n",
    "2. For each training sample x(i):\n",
    "    - Calculate the output value.\n",
    "    - Update the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If the output value matches the true output value, the weights do not need to be updated. If the preceptron predicts a 1 when it should be a zero it means the weights need to be made smaller and vice versa. The weights are updated as follows: \n",
    "\n",
    "<p style='font-size:24px'>$\\Delta w_j:=w_j+\\delta w_j$</p>\n",
    "\n",
    "<p style='font-size:24px'>$\\Delta w_j = \\eta * (target − output ) * x_j$</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This process continues until all of our inputs are classified correctly (convergence). However, this is not the most efficient way to update our weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Adaptive Linear Neuron (Adaline) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In contrast to perceptron learning, the <b>delta rule</b> of the Adaline updates the weights based on an output of a  linear activation function rather than a unit step function --> The output is a continuous value rather than a 1 or 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### What does this allow us to do? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This allows us to differentiate it and in turn define a cost function : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<p style=\"font-size: 20px\">$J(w)= \\frac{1}{2}\\sum_{i} (target^{(i)}−output^{(i)})^2$</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If you recall from linear regression the amount we adjust our weights (called coefficients in linear regression) is dependent on how steep our cost curve for a given set of weights. We can determine \"steepness\" by taking the partial derivate of the cost function $J(w)$ with respect to weight ($\\frac{\\partial J}{\\partial w}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<p style=\"font-size: 20px\">$\\frac{\\partial J}{\\partial w_j} = \\sum_{i}(target^{(i)}−output^{(i)})* x_{j}^{(i)}$</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If you want to see this derivation - visit the first link in the resources section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We multiply this value by a learning rate as we did before to soften the change and add this to our previous weight. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<p style=\"font-size: 20px\">$\\Delta w = \\eta * \\sum_{i}(target^{(i)}−output^{(i)})* x_{j}^{(i)}$</p>\n",
    "<p style=\"font-size: 20px\">$w := w + \\Delta w $</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Next Up : Multilayer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How do we deal with data that is not linearly separable? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://sebastianraschka.com/Articles/2015_singlelayer_neurons.html#the-perceptron-learning-rule \n",
    "\n",
    "https://www.youtube.com/watch?v=ntKn5TPHHAk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
